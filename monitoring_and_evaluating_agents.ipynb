{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlenaVN/Agents_cource_notebooks/blob/main/monitoring_and_evaluating_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDcqoCesUj6q"
      },
      "source": [
        "# Bonus Unit 1: Observability and Evaluation of Agents\n",
        "\n",
        "In this tutorial, we will learn how to **monitor the internal steps (traces) of our AI agent** and **evaluate its performance** using open-source observability tools.\n",
        "\n",
        "The ability to observe and evaluate an agentâ€™s behavior is essential for:\n",
        "- Debugging issues when tasks fail or produce suboptimal results\n",
        "- Monitoring costs and performance in real-time\n",
        "- Improving reliability and safety through continuous feedback\n",
        "\n",
        "This notebook is part of the [Hugging Face Agents Course](https://www.hf.co/learn/agents-course/unit1/introduction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTjrXwEjUj6u"
      },
      "source": [
        "## Step 0: Install the Required Libraries\n",
        "\n",
        "We will need a few libraries that allow us to run, monitor, and evaluate our agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s9wYC6qDUj6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8c8d80-66ea-48c9-e942-9a5dae042bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langfuse\n",
            "  Downloading langfuse-3.12.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting openinference-instrumentation-smolagents\n",
            "  Downloading openinference_instrumentation_smolagents-0.1.21-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-6.3.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smolagents[telemetry]\n",
            "  Downloading smolagents-1.24.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting backoff>=1.10.0 (from langfuse)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: httpx<1.0,>=0.15.4 in /usr/local/lib/python3.12/dist-packages (from langfuse) (0.28.1)\n",
            "Requirement already satisfied: openai>=0.27.8 in /usr/local/lib/python3.12/dist-packages (from langfuse) (2.14.0)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse) (1.37.0)\n",
            "Requirement already satisfied: packaging<26.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langfuse) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /usr/local/lib/python3.12/dist-packages (from langfuse) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langfuse) (2.32.4)\n",
            "Collecting wrapt<2.0,>=1.14 (from langfuse)\n",
            "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.31.2 in /usr/local/lib/python3.12/dist-packages (from smolagents[telemetry]) (0.36.0)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.12/dist-packages (from smolagents[telemetry]) (13.9.4)\n",
            "Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from smolagents[telemetry]) (3.1.6)\n",
            "Requirement already satisfied: pillow>=10.0.1 in /usr/local/lib/python3.12/dist-packages (from smolagents[telemetry]) (11.3.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from smolagents[telemetry]) (1.2.1)\n",
            "Collecting arize-phoenix (from smolagents[telemetry])\n",
            "  Downloading arize_phoenix-12.30.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from smolagents[telemetry])\n",
            "  Downloading opentelemetry_exporter_otlp-1.39.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting openinference-instrumentation>=0.1.27 (from openinference-instrumentation-smolagents)\n",
            "  Downloading openinference_instrumentation-0.1.42-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting openinference-semantic-conventions>=0.1.17 (from openinference-instrumentation-smolagents)\n",
            "  Downloading openinference_semantic_conventions-0.1.25-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting opentelemetry-instrumentation (from openinference-instrumentation-smolagents)\n",
            "  Downloading opentelemetry_instrumentation-0.60b1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-smolagents) (0.58b0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-smolagents) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-23.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Collecting gradio-client==2.0.3 (from gradio)\n",
            "  Downloading gradio_client-2.0.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.21)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.31.2->smolagents[telemetry]) (1.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.27.8->langfuse) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.27.8->langfuse) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=0.27.8->langfuse) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.37.0)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langfuse) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langfuse) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->smolagents[telemetry]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->smolagents[telemetry]) (2.19.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Collecting aioitertools (from arize-phoenix->smolagents[telemetry])\n",
            "  Downloading aioitertools-0.13.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from arize-phoenix->smolagents[telemetry]) (0.22.1)\n",
            "Requirement already satisfied: alembic<2,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from arize-phoenix->smolagents[telemetry]) (1.17.2)\n",
            "Collecting arize-phoenix-client>=1.27.2 (from arize-phoenix->smolagents[telemetry])\n",
            "  Downloading arize_phoenix_client-1.27.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting arize-phoenix-evals>=2.8.0 (from arize-phoenix->smolagents[telemetry])\n",
            "  Downloading arize_phoenix_evals-2.8.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting arize-phoenix-otel>=0.14.0 (from arize-phoenix->smolagents[telemetry])\n",
            "  Downloading arize_phoenix_otel-0.14.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: authlib in /usr/local/lib/python3.12/dist-packages (from arize-phoenix->smolagents[telemetry]) (1.6.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from arize-phoenix->smolagents[telemetry]) (6.2.4)\n",
            "Collecting email-validator (from arize-phoenix->smolagents[telemetry])\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: grpc-interceptor in /usr/local/lib/python3.12/dist-packages (from arize-phoenix->smolagents[telemetry]) (0.15.4)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.12/dist-packages (from arize-phoenix->smolagents[telemetry]) (1.76.0)\n",
            "Collecting jmespath (from arize-phoenix->smolagents[telemetry])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting ldap3 (from arize-phoenix->smolagents[telemetry])\n",
            "  Downloading ldap3-2.9.1-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from arize-phoenix->smolagents[telemetry]) (0.23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from arize-phoenix->smolagents[telemetry]) (5.9.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from arize-phoenix->smolagents[telemetry]) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from arize-phoenix->smolagents[telemetry]) (1.16.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix->smolagents[telemetry]) (2.0.45)\n",
            "Collecting sqlean-py>=3.45.1 (from arize-phoenix->smolagents[telemetry])\n",
            "  Downloading sqlean_py-3.50.4.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting strawberry-graphql==0.287.3 (from arize-phoenix->smolagents[telemetry])\n",
            "  Downloading strawberry_graphql-0.287.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql==0.287.3->arize-phoenix->smolagents[telemetry])\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting lia-web>=0.2.1 (from strawberry-graphql==0.287.3->arize-phoenix->smolagents[telemetry])\n",
            "  Downloading lia_web-0.3.1-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.39.1 (from opentelemetry-exporter-otlp->smolagents[telemetry])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 (from langfuse)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-exporter-otlp-proto-http to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp (from smolagents[telemetry])\n",
            "  Downloading opentelemetry_exporter_otlp-1.39.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.39.0 (from opentelemetry-exporter-otlp->smolagents[telemetry])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 (from langfuse)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.39.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from smolagents[telemetry])\n",
            "  Downloading opentelemetry_exporter_otlp-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.38.0 (from opentelemetry-exporter-otlp->smolagents[telemetry])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 (from langfuse)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from smolagents[telemetry])\n",
            "  Downloading opentelemetry_exporter_otlp-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc==1.37.0 (from opentelemetry-exporter-otlp->smolagents[telemetry])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-instrumentation to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-instrumentation (from openinference-instrumentation-smolagents)\n",
            "  Downloading opentelemetry_instrumentation-0.60b0-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Downloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic<2,>=1.3.0->arize-phoenix->smolagents[telemetry]) (1.3.10)\n",
            "Collecting jsonpath-ng (from arize-phoenix-evals>=2.8.0->arize-phoenix->smolagents[telemetry])\n",
            "  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pystache (from arize-phoenix-evals>=2.8.0->arize-phoenix->smolagents[telemetry])\n",
            "  Downloading pystache-0.6.8-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->smolagents[telemetry]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=2.0.4->sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix->smolagents[telemetry]) (3.3.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.12/dist-packages (from authlib->arize-phoenix->smolagents[telemetry]) (43.0.3)\n",
            "Collecting dnspython>=2.0.0 (from email-validator->arize-phoenix->smolagents[telemetry])\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pyasn1>=0.4.6 in /usr/local/lib/python3.12/dist-packages (from ldap3->arize-phoenix->smolagents[telemetry]) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->arize-phoenix->smolagents[telemetry]) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->arize-phoenix->smolagents[telemetry]) (3.6.0)\n",
            "Collecting cross-web>=0.3.0 (from lia-web>=0.2.1->strawberry-graphql==0.287.3->arize-phoenix->smolagents[telemetry])\n",
            "  Downloading cross_web-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography->authlib->arize-phoenix->smolagents[telemetry]) (2.0.0)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.12/dist-packages (from jsonpath-ng->arize-phoenix-evals>=2.8.0->arize-phoenix->smolagents[telemetry]) (3.11)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography->authlib->arize-phoenix->smolagents[telemetry]) (2.23)\n",
            "Downloading langfuse-3.12.0-py3-none-any.whl (416 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m416.9/416.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openinference_instrumentation_smolagents-0.1.21-py3-none-any.whl (13 kB)\n",
            "Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-6.3.0-py3-none-any.whl (23.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.0/23.0 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-2.0.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading openinference_instrumentation-0.1.42-py3-none-any.whl (30 kB)\n",
            "Downloading openinference_semantic_conventions-0.1.25-py3-none-any.whl (10 kB)\n",
            "Downloading pyarrow-23.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix-12.30.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading strawberry_graphql-0.287.3-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp-1.37.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl (33 kB)\n",
            "Downloading smolagents-1.24.0-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.7/155.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_client-1.27.2-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_evals-2.8.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.8/161.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_otel-0.14.0-py3-none-any.whl (17 kB)\n",
            "Downloading sqlean_py-3.50.4.5-cp312-cp312-manylinux_2_28_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.13.0-py3-none-any.whl (24 kB)\n",
            "Downloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading ldap3-2.9.1-py2.py3-none-any.whl (432 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m432.2/432.2 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lia_web-0.3.1-py3-none-any.whl (5.9 kB)\n",
            "Downloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\n",
            "Downloading pystache-0.6.8-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cross_web-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: wrapt, sqlean-py, pystache, pyarrow, openinference-semantic-conventions, ldap3, jsonpath-ng, jmespath, graphql-core, dnspython, cross-web, backoff, aioitertools, lia-web, email-validator, strawberry-graphql, smolagents, gradio-client, opentelemetry-instrumentation, gradio, datasets, opentelemetry-exporter-otlp-proto-grpc, openinference-instrumentation, opentelemetry-exporter-otlp, openinference-instrumentation-smolagents, langfuse, arize-phoenix-evals, arize-phoenix-otel, arize-phoenix-client, arize-phoenix\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 2.0.1\n",
            "    Uninstalling wrapt-2.0.1:\n",
            "      Successfully uninstalled wrapt-2.0.1\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.14.0\n",
            "    Uninstalling gradio_client-1.14.0:\n",
            "      Successfully uninstalled gradio_client-1.14.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.50.0\n",
            "    Uninstalling gradio-5.50.0:\n",
            "      Successfully uninstalled gradio-5.50.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed aioitertools-0.13.0 arize-phoenix-12.30.0 arize-phoenix-client-1.27.2 arize-phoenix-evals-2.8.0 arize-phoenix-otel-0.14.0 backoff-2.2.1 cross-web-0.4.1 datasets-4.5.0 dnspython-2.8.0 email-validator-2.3.0 gradio-6.3.0 gradio-client-2.0.3 graphql-core-3.2.7 jmespath-1.0.1 jsonpath-ng-1.7.0 langfuse-3.12.0 ldap3-2.9.1 lia-web-0.3.1 openinference-instrumentation-0.1.42 openinference-instrumentation-smolagents-0.1.21 openinference-semantic-conventions-0.1.25 opentelemetry-exporter-otlp-1.37.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 opentelemetry-instrumentation-0.58b0 pyarrow-23.0.0 pystache-0.6.8 smolagents-1.24.0 sqlean-py-3.50.4.5 strawberry-graphql-0.287.3 wrapt-1.17.3\n"
          ]
        }
      ],
      "source": [
        "%pip install langfuse 'smolagents[telemetry]' openinference-instrumentation-smolagents datasets 'smolagents[gradio]' gradio --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPspOQlQUj6x"
      },
      "source": [
        "## Step 1: Instrument Your Agent\n",
        "\n",
        "In this notebook, we will use [Langfuse](https://langfuse.com/) as our observability tool, but you can use **any other OpenTelemetry-compatible service**. The code below shows how to set environment variables for Langfuse (or any OTel endpoint) and how to instrument your smolagent.\n",
        "\n",
        "**Note:** If you are using LlamaIndex or LangGraph, you can find documentation on instrumenting them [here](https://langfuse.com/docs/integrations/llama-index/workflows) and [here](https://langfuse.com/docs/integrations/langchain/example-python-langgraph)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpxvOR2eUj6y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ğŸ‡ªğŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ğŸ‡ºğŸ‡¸ US region\n",
        "\n",
        "# Set your Hugging Face and other tokens/secrets as environment variable\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMttyxGmUj6z"
      },
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. get_client() initializes the Langfuse client using the credentials provided in the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P0u5EDjUj6z",
        "outputId": "79f9674a-7ef6-4f32-e9c0-b1edd56a6081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Langfuse client is authenticated and ready!\n"
          ]
        }
      ],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "# Verify connection\n",
        "if langfuse.auth_check():\n",
        "    print(\"Langfuse client is authenticated and ready!\")\n",
        "else:\n",
        "    print(\"Authentication failed. Please check your credentials and host.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuKh7PB6Uj61",
        "outputId": "fa4c71be-5e3c-4a08-d966-02c9e452767b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attempting to instrument while already instrumented\n"
          ]
        }
      ],
      "source": [
        "from openinference.instrumentation.smolagents import SmolagentsInstrumentor\n",
        "\n",
        "SmolagentsInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcioicWGUj61"
      },
      "source": [
        "## Step 2: Test Your Instrumentation\n",
        "\n",
        "Here is a simple CodeAgent from smolagents that calculates `1+1`. We run it to confirm that the instrumentation is working correctly. If everything is set up correctly, you will see logs/spans in your observability dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGqwfAK-Uj62"
      },
      "outputs": [],
      "source": [
        "from smolagents import InferenceClientModel, CodeAgent\n",
        "\n",
        "# Create a simple agent to test instrumentation\n",
        "agent = CodeAgent(\n",
        "    tools=[],\n",
        "    model=InferenceClientModel()\n",
        ")\n",
        "\n",
        "agent.run(\"1+1=\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0iUYXu-Uj63"
      },
      "source": [
        "## Step 3: Observe and Evaluate a More Complex Agent\n",
        "\n",
        "Now that you have confirmed your instrumentation works, let's try a more complex query so we can see how advanced metrics (token usage, latency, costs, etc.) are tracked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc2GyouAUj63"
      },
      "outputs": [],
      "source": [
        "from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)\n",
        "\n",
        "search_tool = DuckDuckGoSearchTool()\n",
        "agent = CodeAgent(tools=[search_tool], model=InferenceClientModel())\n",
        "\n",
        "agent.run(\"How many Rubik's Cubes could you fit inside the Notre Dame Cathedral?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKgFunlSUj64"
      },
      "source": [
        "## Online Evaluation\n",
        "\n",
        "In the previous section, we learned about the difference between online and offline evaluation. Now, we will see how to monitor your agent in production and evaluate it live.\n",
        "\n",
        "### Common Metrics to Track in Production\n",
        "\n",
        "1. **Costs** â€” The smolagents instrumentation captures token usage, which you can transform into approximate costs by assigning a price per token.\n",
        "2. **Latency** â€” Observe the time it takes to complete each step, or the entire run.\n",
        "3. **User Feedback** â€” Users can provide direct feedback (thumbs up/down) to help refine or correct the agent.\n",
        "4. **LLM-as-a-Judge** â€” Use a separate LLM to evaluate your agentâ€™s output in near real-time (e.g., checking for toxicity or correctness).\n",
        "\n",
        "Below, we show examples of these metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UQYT2BKUj65"
      },
      "source": [
        "#### 1. Costs\n",
        "\n",
        "Below is a screenshot showing usage for `Qwen2.5-Coder-32B-Instruct` calls. This is useful to see costly steps and optimize your agent.\n",
        "\n",
        "![Costs](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-costs.png)\n",
        "\n",
        "_[Link to the trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1ac33b89ffd5e75d4265b62900c348ed?timestamp=2025-03-07T13%3A45%3A09.149Z&display=preview)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf-HmVe5Uj66"
      },
      "source": [
        "#### 3. Additional Attributes\n",
        "\n",
        "You may also pass additional attributes to your spans. These can include `user_id`, `tags`, `session_id`, and custom metadata. Enriching traces with these details is important for analysis, debugging, and monitoring of your applicationâ€™s behavior across different users or sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z73PA4TrUj66"
      },
      "outputs": [],
      "source": [
        "from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)\n",
        "\n",
        "search_tool = DuckDuckGoSearchTool()\n",
        "agent = CodeAgent(\n",
        "    tools=[search_tool],\n",
        "    model=InferenceClientModel()\n",
        ")\n",
        "\n",
        "with langfuse.start_as_current_span(\n",
        "    name=\"Smolagent-Trace\",\n",
        "    ) as span:\n",
        "\n",
        "    # Run your application here\n",
        "    response = agent.run(\"What is the capital of Germany?\")\n",
        "\n",
        "    # Pass additional attributes to the span\n",
        "    span.update_trace(\n",
        "        input=\"What is the capital of Germany?\",\n",
        "        output=response,\n",
        "        user_id=\"smolagent-user-123\",\n",
        "        session_id=\"smolagent-session-123456789\",\n",
        "        tags=[\"city-question\", \"testing-agents\"],\n",
        "        metadata={\"email\": \"user@langfuse.com\"},\n",
        "        )\n",
        "\n",
        "# Flush events in short-lived applications\n",
        "langfuse.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MkGvH4LUj67"
      },
      "source": [
        "#### 4. User Feedback\n",
        "\n",
        "If your agent is embedded into a user interface, you can record direct user feedback (like a thumbs-up/down in a chat UI). Below is an example using [Gradio](https://gradio.app/) to embed a chat with a simple feedback mechanism.\n",
        "\n",
        "In the code snippet below, when a user sends a chat message, we capture the trace in Langfuse. If the user likes/dislikes the last answer, we attach a score to the trace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8Tdt8FiUj68"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from smolagents import (CodeAgent, InferenceClientModel)\n",
        "from langfuse import get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "model = InferenceClientModel()\n",
        "agent = CodeAgent(tools=[], model=model, add_base_tools=True)\n",
        "\n",
        "trace_id = None\n",
        "\n",
        "def respond(prompt, history):\n",
        "    with langfuse.start_as_current_span(\n",
        "        name=\"Smolagent-Trace\"):\n",
        "\n",
        "        # Run your application here\n",
        "        output = agent.run(prompt)\n",
        "\n",
        "        global trace_id\n",
        "        trace_id = langfuse.get_current_trace_id()\n",
        "\n",
        "    history.append({\"role\": \"assistant\", \"content\": str(output)})\n",
        "    return history\n",
        "\n",
        "def handle_like(data: gr.LikeData):\n",
        "    # For demonstration, we map user feedback to a 1 (like) or 0 (dislike)\n",
        "    if data.liked:\n",
        "        langfuse.create_score(\n",
        "            value=1,\n",
        "            name=\"user-feedback\",\n",
        "            trace_id=trace_id\n",
        "        )\n",
        "    else:\n",
        "        langfuse.create_score(\n",
        "            value=0,\n",
        "            name=\"user-feedback\",\n",
        "            trace_id=trace_id\n",
        "        )\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"Chat\", type=\"messages\")\n",
        "    prompt_box = gr.Textbox(placeholder=\"Type your message...\", label=\"Your message\")\n",
        "\n",
        "    # When the user presses 'Enter' on the prompt, we run 'respond'\n",
        "    prompt_box.submit(\n",
        "        fn=respond,\n",
        "        inputs=[prompt_box, chatbot],\n",
        "        outputs=chatbot\n",
        "    )\n",
        "\n",
        "    # When the user clicks a 'like' button on a message, we run 'handle_like'\n",
        "    chatbot.like(handle_like, None, None)\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEI6qyIaUj69"
      },
      "outputs": [],
      "source": [
        "# Example: Checking if the agentâ€™s output is toxic or not.\n",
        "from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)\n",
        "\n",
        "search_tool = DuckDuckGoSearchTool()\n",
        "agent = CodeAgent(tools=[search_tool], model=InferenceClientModel())\n",
        "\n",
        "agent.run(\"Can eating carrots improve your vision?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4SpUec8Uj6-"
      },
      "source": [
        "## Offline Evaluation\n",
        "\n",
        "Online evaluation is essential for live feedback, but you also need **offline evaluation**â€”systematic checks before or during development. This helps maintain quality and reliability before rolling changes into production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5I9vJmuUj6-"
      },
      "source": [
        "### Dataset Evaluation\n",
        "\n",
        "In offline evaluation, you typically:\n",
        "1. Have a benchmark dataset (with prompt and expected output pairs)\n",
        "2. Run your agent on that dataset\n",
        "3. Compare outputs to the expected results or use an additional scoring mechanism\n",
        "\n",
        "Below, we demonstrate this approach with the [GSM8K dataset](https://huggingface.co/datasets/gsm8k), which contains math questions and solutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2rLJgwcUj6_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Fetch GSM8K from Hugging Face\n",
        "dataset = load_dataset(\"openai/gsm8k\", 'main', split='train')\n",
        "df = pd.DataFrame(dataset)\n",
        "print(\"First few rows of GSM8K dataset:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhSUI4bKUj6_"
      },
      "source": [
        "Next, we create a dataset entity in Langfuse to track the runs. Then, we add each item from the dataset to the system. (If youâ€™re not using Langfuse, you might simply store these in your own database or local file for analysis.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KpJAxFYUj6_"
      },
      "outputs": [],
      "source": [
        "from langfuse import get_client\n",
        "langfuse = get_client()\n",
        "\n",
        "langfuse_dataset_name = \"gsm8k_dataset_huggingface\"\n",
        "\n",
        "# Create a dataset in Langfuse\n",
        "langfuse.create_dataset(\n",
        "    name=langfuse_dataset_name,\n",
        "    description=\"GSM8K benchmark dataset uploaded from Huggingface\",\n",
        "    metadata={\n",
        "        \"date\": \"2025-03-10\",\n",
        "        \"type\": \"benchmark\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIp4ImvrUj7A"
      },
      "outputs": [],
      "source": [
        "for idx, row in df.iterrows():\n",
        "    langfuse.create_dataset_item(\n",
        "        dataset_name=langfuse_dataset_name,\n",
        "        input={\"text\": row[\"question\"]},\n",
        "        expected_output={\"text\": row[\"answer\"]},\n",
        "        metadata={\"source_index\": idx}\n",
        "    )\n",
        "    if idx >= 9: # Upload only the first 10 items for demonstration\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EN40WZ4Uj7A"
      },
      "source": [
        "#### Running the Agent on the Dataset\n",
        "\n",
        "We define a helper function `run_smolagent()` that:\n",
        "1. Starts a Langfuse span\n",
        "2. Runs our agent on the prompt\n",
        "3. Records the trace ID in Langfuse\n",
        "\n",
        "Then, we loop over each dataset item, run the agent, and link the trace to the dataset item. We can also attach a quick evaluation score if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLtXcRGfUj7B"
      },
      "outputs": [],
      "source": [
        "from opentelemetry.trace import format_trace_id\n",
        "from smolagents import (CodeAgent, InferenceClientModel, LiteLLMModel)\n",
        "from langfuse import get_client\n",
        "\n",
        "langfuse = get_client()\n",
        "\n",
        "\n",
        "# Example: using InferenceClientModel or LiteLLMModel to access openai, anthropic, gemini, etc. models:\n",
        "model = InferenceClientModel()\n",
        "\n",
        "agent = CodeAgent(\n",
        "    tools=[],\n",
        "    model=model,\n",
        "    add_base_tools=True\n",
        ")\n",
        "\n",
        "dataset_name = \"gsm8k_dataset_huggingface\"\n",
        "current_run_name = \"smolagent-notebook-run-01\" # Identifies this specific evaluation run\n",
        "\n",
        "# Assume 'run_smolagent' is your instrumented application function\n",
        "def run_smolagent(question):\n",
        "    with langfuse.start_as_current_generation(name=\"qna-llm-call\") as generation:\n",
        "        # Simulate LLM call\n",
        "        result = agent.run(question)\n",
        "\n",
        "        # Update the trace with the input and output\n",
        "        generation.update_trace(\n",
        "            input= question,\n",
        "            output=result,\n",
        "        )\n",
        "\n",
        "        return result\n",
        "\n",
        "dataset = langfuse.get_dataset(name=dataset_name) # Fetch your pre-populated dataset\n",
        "\n",
        "for item in dataset.items:\n",
        "\n",
        "    # Use the item.run() context manager\n",
        "    with item.run(\n",
        "        run_name=current_run_name,\n",
        "        run_metadata={\"model_provider\": \"Hugging Face\", \"temperature_setting\": 0.7},\n",
        "        run_description=\"Evaluation run for GSM8K dataset\"\n",
        "    ) as root_span: # root_span is the root span of the new trace for this item and run.\n",
        "        # All subsequent langfuse operations within this block are part of this trace.\n",
        "\n",
        "        # Call your application logic\n",
        "        generated_answer = run_smolagent(question=item.input[\"text\"])\n",
        "\n",
        "        print(item.input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL8qWJlMUj7B"
      },
      "source": [
        "## Final Thoughts\n",
        "\n",
        "In this notebook, we covered how to:\n",
        "1. **Set up Observability** using smolagents + OpenTelemetry exporters\n",
        "2. **Check Instrumentation** by running a simple agent\n",
        "3. **Capture Detailed Metrics** (cost, latency, etc.) through an observability tools\n",
        "4. **Collect User Feedback** via a Gradio interface\n",
        "5. **Use LLM-as-a-Judge** to automatically evaluate outputs\n",
        "6. **Perform Offline Evaluation** with a benchmark dataset\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}